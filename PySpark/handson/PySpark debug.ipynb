{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d03cb07-b794-45c4-82fc-0fa4fec56897",
   "metadata": {},
   "source": [
    "PySpark debuggin code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e009e-c9b2-40fa-8d52-fa10f804e7a1",
   "metadata": {},
   "source": [
    "1. Enable spark logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14e64f-f06c-4ca0-8a19-ec7d5a19800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") \n",
    "#options: all,debug,info,warn,error\n",
    "#helps reduce or increase log verbosity for better debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eaa46b-ee36-4199-a837-d64766550b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Check spark configuration at runtime\n",
    "spark.sparkContext.getConf().getAll()\n",
    "#Useful to verify memory, cores, shuffle config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e25e39-0f39-4a8a-9e37-e510f1135a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Executor memory debug\n",
    "-- conf spark.executor.memory=2g\n",
    "-- conf spark.driver.memory=1g\n",
    "#used when facing OutOfMemory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4096ac9a-e110-4b6b-818e-86cbd0a102e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. shuffle debugging - too much shuffle\n",
    "df = df.repartition(100,\"key\") #Before groupBy or join\n",
    "#Avoids default small partitions that cause skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31cf4c-95cb-4375-9968-b391b16f273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. handling skewed join keys\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = big_df.join(broadcast(small_df),\"key\")\n",
    "Fixes long tasks due to skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68f95d-4de3-4334-97f5-dcc9b755e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.catch read failures\n",
    "try:\n",
    "    df=spark.read.csv(\"path/does/not/exist\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to read:\",e)\n",
    "#Good for detecting bad paths or file formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a6b1e-873f-4f23-933e-9bff10164359",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Memory usage via executors tab\n",
    "Use Spark UI at localhost:4040>Executors tab\n",
    "Diagnoses executor memory issues visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d1a93-c103-41c2-bf8e-481059657bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Monitor Job execution time\n",
    "from time import time\n",
    "start = time()\n",
    "df.count()\n",
    "print(\"Execution Time:\",time()-start)\n",
    "Useful to measure slow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdeccd1-0dc1-487f-a105-e6e89a6193b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Use .persist() wisely\n",
    "df = df.filter(\"status = 'active'\").persist()\n",
    "Avoid caching raw or large unfiltered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c48640-9072-4706-9f5c-30e542b1c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. analyze task duration\n",
    "view task skew in Spark UI>Stages>Tasks\n",
    "Useful to detect long-running tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f8f96-d4d3-46f4-a1cd-1975b0cf6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Avoid exploding memory in collect\n",
    "#bad\n",
    "all_rows=df.collect()\n",
    "#better\n",
    "df.show(10)\n",
    "Prevents memory issues on the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e700055-3dfe-44d1-8c94-e98f218969e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. check number of partitions\n",
    "print(df.rdd.getNumPartitions())\n",
    "Too few partitions cause bottlenecks. Too many = overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a482d-aff7-48f2-9ae3-045f6cf72305",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. debug join type issues\n",
    "df1.join(df2,\"id\",\"inner\").explain(True)\n",
    "Check for broadcast hint or sort-merge join issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3116a-45e6-4b39-a674-6fd84132ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. investigate lazy evaluation problems\n",
    "df.printSchema() #doesn't trigger job\n",
    "df.count() #triggers full execution\n",
    "use actions to debug transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0250e52-f7cd-4910-ba1a-161dbaca28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Track Failed Jobs\n",
    "Use Spark UI>Jobs tab>click failed job>review stderr logs\n",
    "Best for tracking OOM, file read, schema mismatch errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
