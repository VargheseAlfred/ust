{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\",\"ParallelizeExample\")"
      ],
      "metadata": {
        "id": "hBcLGnZJNmnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5053dd49-f398-4e75-d15a-8b6f1e41fc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=ParallelizeExample, master=local) created by __init__ at <ipython-input-2-d9be8aec9465>:2 ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d9be8aec9465>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ParallelizeExample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    450\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=ParallelizeExample, master=local) created by __init__ at <ipython-input-2-d9be8aec9465>:2 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(\"/content/EVM.txt\")"
      ],
      "metadata": {
        "id": "ZgV2kKgkNsJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGEQZ_00NyvY",
        "outputId": "f78c4ccd-ac59-48a9-f785-b6ce5856917e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1y5qW3fN5hU",
        "outputId": "9292dcf0-f2b6-4ca0-df60-27405457bc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TutfGfDXOIJ9",
        "outputId": "ceecca27-12cc-4ec0-98b6-0d783e99bd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhYxm0irOL50",
        "outputId": "b499fc68-3c95-4184-c02d-104297acea77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF',\n",
              " 'BJP RJD CPIM AAP AIDMK ADMK UDF BJP RJD CPIM AAP AIDMK ADMK UDF']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rdd_split = rdd.flatMap(lambda line: line.split(\" \"))\n",
        "rdd_split.collect()\n",
        "rdd_split.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVtXl3z8OQ0l",
        "outputId": "92ff72cf-706c-4757-f7ca-afd06e6a3d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'BJP': 12,\n",
              "             'RJD': 12,\n",
              "             'CPIM': 12,\n",
              "             'AAP': 12,\n",
              "             'AIDMK': 12,\n",
              "             'ADMK': 12,\n",
              "             'UDF': 12})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd = sc.textFile(\"/content/EVM.txt\")\n",
        "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "pair_rdd = words_rdd.map(lambda word: (word, 1))\n",
        "word_count_rdd = pair_rdd.reduceByKey(lambda a, b: a + b)\n",
        "word_count_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nrb12a6RFFD",
        "outputId": "b8ea8960-ee3d-4f3f-fa56-f47e490b5ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('BJP', 12),\n",
              " ('RJD', 12),\n",
              " ('CPIM', 12),\n",
              " ('AAP', 12),\n",
              " ('AIDMK', 12),\n",
              " ('ADMK', 12),\n",
              " ('UDF', 12)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp_data = [\n",
        "\n",
        "[\"001\",\"101\", \"John Doe\", \"30\", \"Male\", \"50000\", \"2015-01-01\"],\n",
        "\n",
        "[\"002\", \"101\", \"Jane Smith\", \"25\", \"Female\", \"45000\", \"2016-02-15\"],\n",
        "\n",
        "[\"003\", \"102\", \"Bob Brown\", \"35\", \"Male\", \"55000\", \"2014-05-01\"],\n",
        "\n",
        "[\"004\", \"102\", \"Alice Lee\", \"28\", \"Female\", \"48000\", \"2017-09-30\"],\n",
        "\n",
        "[\"005\", \"103\", \"Jack Chan\", \"40\", \"Male\", \"60000\", \"2013-04-01\"],\n",
        "\n",
        "[\"006\", \"103\", \"Jill Wong\", \"32\", \"Female\", \"52000\", \"2018-07-01\"],\n",
        "\n",
        "[\"007\", \"101\", \"James Johnson\", \"42\", \"Male\", \"70000\", \"2012-03-15\"],\n",
        "\n",
        "[\"008\", \"102\", \"Kate Kim\", \"29\", \"Female\", \"51000\", \"2019-10-01\"],\n",
        "\n",
        "[\"009\", \"103\", \"Tom Tan\", \"33\", \"Male\", \"58000\", \"2016-06-01\"],\n",
        "\n",
        "[\"010\", \"104\", \"Lisa Lee\", \"27\", \"Female\", \"47000\",\"2018-08-01\"],\n",
        "\n",
        "[\"011\", \"104\", \"David Park\", \"38\", \"Male\", \"65000\",\"2015-11-01\"],\n",
        "\n",
        "[\"012\", \"105\", \"Susan Chen\", \"31\", \"Female\", \"54000\",\"2017-02-15\"],\n",
        "\n",
        "[\"013\", \"106\", \"Brian Kim\", \"45\", \"Male\", \"75000\",\"2011-07-01\"],\n",
        "\n",
        "[\"014\", \"107\", \"Emily Lee\", \"26\", \"Female\", \"46000\", \"2019-01-01\"],\n",
        "\n",
        "[\"015\", \"106\", \"Michael Lee\", \"37\", \"Male\", \"63000\", \"2014-09-30\"],\n",
        "\n",
        "[\"016\", \"107\", \"Kelly Zhang\", \"30\", \"Female\", \"49000\", \"2016-04-01\"],\n",
        "\n",
        "[\"017\", \"105\", \"George Wang\", \"34\", \"Male\", \"57000\", \"2016-03-15\"]\n",
        "]"
      ],
      "metadata": {
        "id": "iCJ-3D19SrJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = \"employee_id string, department_id string,name string,age string,gender string,salary string,hire_date string\""
      ],
      "metadata": {
        "id": "wn2ZOgWXYo_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"emp\").getOrCreate()"
      ],
      "metadata": {
        "id": "w7rDQJpKZfWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp = spark.createDataFrame(data=emp_data,schema=columns)"
      ],
      "metadata": {
        "id": "_S3xofU9Y47Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCxEJGU2Y-R_",
        "outputId": "47499539-2181-4be1-d9ad-9765c9b71d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2016-04-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.printSchema()"
      ],
      "metadata": {
        "id": "CaU02BC8adoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe48857-6c3c-49f6-e609-8ec289e11ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_id: string (nullable = true)\n",
            " |-- department_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- hire_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the schema manually with nullable"
      ],
      "metadata": {
        "id": "_lt9pONmFr-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
      ],
      "metadata": {
        "id": "O4-ZzePHGKuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"name\",StringType(),nullable=False),\n",
        "    StructField(\"age\",IntegerType(),nullable=False),\n",
        "    StructField(\"city\",StringType(),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "qj4O8tVvGXdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Alice\",25,\"New York\"),\n",
        "    (\"Bob\",30,\"Los Angeles\"),\n",
        "]\n"
      ],
      "metadata": {
        "id": "F1KC8BMiHi1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = spark.createDataFrame(data,schema)\n",
        "dummy_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ruI_q8HzT9",
        "outputId": "5ab4a35d-874d-4c5c-a24e-df0caab64f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- age: integer (nullable = false)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t4YNXZgH_p1",
        "outputId": "3af916ab-10ed-4b9c-89d3-2bbe619800b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----------+\n",
            "| name|age|       city|\n",
            "+-----+---+-----------+\n",
            "|Alice| 25|   New York|\n",
            "|  Bob| 30|Los Angeles|\n",
            "+-----+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.select(\"name\",\"age\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pcuj1XlKICdz",
        "outputId": "2c532e67-f5be-4503-b933-24dc481156cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+\n",
            "|      name|age|\n",
            "+----------+---+\n",
            "|  John Doe| 30|\n",
            "|Jane Smith| 25|\n",
            "| Bob Brown| 35|\n",
            "| Alice Lee| 28|\n",
            "| Jack Chan| 40|\n",
            "+----------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.filter(emp.age>30).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noui01ulIPKy",
        "outputId": "463a93e0-6e2a-4dae-e9d9-3ecbaaecd9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.orderBy(emp.salary).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzMfx64RIsMK",
        "outputId": "98d6f186-b7d8-47c6-f54f-0d355027b171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2016-04-01|\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emp.orderBy(emp.salary.desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzDxplefJTXV",
        "outputId": "7ffe98c3-4127-4f17-d8d7-e014f2e7c1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|employee_id|department_id|         name|age|gender|salary| hire_date|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n",
            "|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n",
            "|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n",
            "|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n",
            "|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n",
            "|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n",
            "|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n",
            "|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n",
            "|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n",
            "|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n",
            "|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n",
            "|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n",
            "|        016|          107|  Kelly Zhang| 30|Female| 49000|2016-04-01|\n",
            "|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n",
            "|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n",
            "|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n",
            "|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n",
            "+-----------+-------------+-------------+---+------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark sql"
      ],
      "metadata": {
        "id": "mYVTzVBqL4bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ev_data = spark.read.csv('/content/EV_Dataset.csv',header=True)\n",
        "ev_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "xCcojI91K_JG",
        "outputId": "6a47557b-d650-4668-bc96-429e987e993c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/EV_Dataset.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-e93613351135>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/EV_Dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mev_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/EV_Dataset.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading csv with InferSchema\n",
        "\n",
        "The inferSchema=True option automatically detects data types."
      ],
      "metadata": {
        "id": "FPKe1GOhNy6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/drive/MyDrive/datasets/EV_Dataset.csv',header=True,inferSchema=True)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIa_HOD5OHkJ",
        "outputId": "f0a25b15-dd39-4dc2-d2a7-7ba4962f590c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Year: double (nullable = true)\n",
            " |-- Month_Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Vehicle_Class: string (nullable = true)\n",
            " |-- Vehicle_Category: string (nullable = true)\n",
            " |-- Vehicle_Type: string (nullable = true)\n",
            " |-- EV_Sales_Quantity: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read mode in pyspark\n",
        "\n",
        "Read mode defines how Spark handles corrupted or missing data.\n",
        "\n",
        "Default Read Mode (PERMISSIVE)\n",
        "Default mode: loads bad records as NULL values instead of failing.\n",
        "\n",
        "If any missing values was there then it will come as NULL"
      ],
      "metadata": {
        "id": "fFdSYU_NPwfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('/content/drive/MyDrive/datasets/EV_Dataset.csv',header=True, mode='PERMISSIVE')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwG_ZMxfQadP",
        "outputId": "41c78cf5-9fa8-497c-92ce-76df33da943b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|  Year|Month_Name|    Date|         State|       Vehicle_Class|Vehicle_Category|   Vehicle_Type|EV_Sales_Quantity|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     ADAPTED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|AGRICULTURAL TRACTOR|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           AMBULANCE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh| ARTICULATED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|                 BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            CASH VAN|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|CRANE MOUNTED VEH...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EDUCATIONAL INSTI...|             Bus|Institution Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EXCAVATOR (COMMER...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           FORK LIFT|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|       GOODS CARRIER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           HARVESTER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            MAXI CAB|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     M-CYCLE/SCOOTER|      2-Wheelers|    2W_Personal|              1.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAB|      4-Wheelers|      4W_Shared|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAR|      4-Wheelers|    4W_Personal|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            OMNI BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|PRIVATE SERVICE V...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|    RECOVERY VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|THREE WHEELER (PA...|      3-Wheelers|      3W_Shared|              0.0|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop the missing values(now working)"
      ],
      "metadata": {
        "id": "TLZGMBIiRAkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.csv('/content/drive/MyDrive/datasets/EV_Dataset.csv',header=True, mode='DROPMALFORMED')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv--qZyfRH25",
        "outputId": "21bbd8f1-39ef-4f2e-d7e4-c85970dbc9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|  Year|Month_Name|    Date|         State|       Vehicle_Class|Vehicle_Category|   Vehicle_Type|EV_Sales_Quantity|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     ADAPTED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|AGRICULTURAL TRACTOR|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           AMBULANCE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh| ARTICULATED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|                 BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            CASH VAN|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|CRANE MOUNTED VEH...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EDUCATIONAL INSTI...|             Bus|Institution Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EXCAVATOR (COMMER...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           FORK LIFT|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|       GOODS CARRIER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           HARVESTER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            MAXI CAB|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     M-CYCLE/SCOOTER|      2-Wheelers|    2W_Personal|              1.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAB|      4-Wheelers|      4W_Shared|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAR|      4-Wheelers|    4W_Personal|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            OMNI BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|PRIVATE SERVICE V...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|    RECOVERY VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|THREE WHEELER (PA...|      3-Wheelers|      3W_Shared|              0.0|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the DataFrame to Parquet"
      ],
      "metadata": {
        "id": "hnBIftgKVWgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode('overwrite').parquet('/content/drive/MyDrive/datasets/parquet')"
      ],
      "metadata": {
        "id": "Tb4w_hCnVelg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet = spark.read.parquet('/content/drive/MyDrive/datasets/parquet')\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLtcMJDaVty8",
        "outputId": "7f6ad7b9-5107-4258-ca61-49da4dc2bc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|  Year|Month_Name|    Date|         State|       Vehicle_Class|Vehicle_Category|   Vehicle_Type|EV_Sales_Quantity|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     ADAPTED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|AGRICULTURAL TRACTOR|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           AMBULANCE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh| ARTICULATED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|                 BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            CASH VAN|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|CRANE MOUNTED VEH...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EDUCATIONAL INSTI...|             Bus|Institution Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EXCAVATOR (COMMER...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           FORK LIFT|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|       GOODS CARRIER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           HARVESTER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            MAXI CAB|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     M-CYCLE/SCOOTER|      2-Wheelers|    2W_Personal|              1.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAB|      4-Wheelers|      4W_Shared|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAR|      4-Wheelers|    4W_Personal|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            OMNI BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|PRIVATE SERVICE V...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|    RECOVERY VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|THREE WHEELER (PA...|      3-Wheelers|      3W_Shared|              0.0|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the dataframe into ORC file"
      ],
      "metadata": {
        "id": "aWDFLdEfZAAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.orc('/content/drive/MyDrive/datasets/orc',mode='overwrite')"
      ],
      "metadata": {
        "id": "xIP8Jp-hZEf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the file from ORC"
      ],
      "metadata": {
        "id": "wrV1kRNpZQFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_orc = spark.read.orc('/content/drive/MyDrive/datasets/orc')\n",
        "df_orc.show()\n",
        "df_orc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydHDnj-zZNrJ",
        "outputId": "6d3b3065-f936-4eca-a6ac-8ff84dd62502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|  Year|Month_Name|    Date|         State|       Vehicle_Class|Vehicle_Category|   Vehicle_Type|EV_Sales_Quantity|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     ADAPTED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|AGRICULTURAL TRACTOR|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           AMBULANCE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh| ARTICULATED VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|                 BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            CASH VAN|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|CRANE MOUNTED VEH...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EDUCATIONAL INSTI...|             Bus|Institution Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|EXCAVATOR (COMMER...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           FORK LIFT|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|       GOODS CARRIER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           HARVESTER|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            MAXI CAB|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|     M-CYCLE/SCOOTER|      2-Wheelers|    2W_Personal|              1.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAB|      4-Wheelers|      4W_Shared|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|           MOTOR CAR|      4-Wheelers|    4W_Personal|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|            OMNI BUS|             Bus|            Bus|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|PRIVATE SERVICE V...|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|    RECOVERY VEHICLE|          Others|         Others|              0.0|\n",
            "|2014.0|       jan|1/1/2014|Andhra Pradesh|THREE WHEELER (PA...|      3-Wheelers|      3W_Shared|              0.0|\n",
            "+------+----------+--------+--------------+--------------------+----------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- Year: string (nullable = true)\n",
            " |-- Month_Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Vehicle_Class: string (nullable = true)\n",
            " |-- Vehicle_Category: string (nullable = true)\n",
            " |-- Vehicle_Type: string (nullable = true)\n",
            " |-- EV_Sales_Quantity: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = [\n",
        "    {\"employee_id\":1,\"name\":\"Alice\",\"details\":{\"department\":\"HR\",\"salary\":5000}},\n",
        "    {\"employee_id\":2,\"name\":\"Bob\",\"details\":{\"department\":\"IT\",\"salary\":6000}},\n",
        "    {\"employee_id\":3,\"name\":\"Charlie\",\"details\":{\"department\":\"Finance\",\"salary\":7000}}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(json_data)\n",
        "df.write.mode('overwrite').json('/content/drive/MyDrive/datasets/json')"
      ],
      "metadata": {
        "id": "caFfybCYZlLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCvr461scWe9",
        "outputId": "da9b7a05-d1c9-4277-b5c8-84065d525ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+-------+\n",
            "|             details|employee_id|   name|\n",
            "+--------------------+-----------+-------+\n",
            "|{salary -> 5000, ...|          1|  Alice|\n",
            "|{salary -> 6000, ...|          2|    Bob|\n",
            "|{salary -> 7000, ...|          3|Charlie|\n",
            "+--------------------+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_flattened = df.select(\n",
        "    \"employee_id\",\n",
        "    \"name\",\n",
        "    \"details.department\",\n",
        "    \"details.salary\"\n",
        ")\n",
        "\n",
        "df_flattened.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4uy3E1mcrHs",
        "outputId": "621d27f5-81e0-494b-a9c3-314bf45388ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----------+------+\n",
            "|employee_id|   name|department|salary|\n",
            "+-----------+-------+----------+------+\n",
            "|          1|  Alice|        HR|  5000|\n",
            "|          2|    Bob|        IT|  6000|\n",
            "|          3|Charlie|   Finance|  7000|\n",
            "+-----------+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nICwsHOHeTuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IvomxNwyq4nl",
        "outputId": "71cc4437-bc1a-4395-b235-a0bb41ba73f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f3f8ca0c410>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://63545838cb9e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ParallelizeExample</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4vFflrirMIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}